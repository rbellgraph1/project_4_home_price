{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import LeakyReLU\n",
    "from tensorflow.keras import layers\n",
    "import configparser\n",
    "import psycopg2\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 150, 'display.max_rows', 255)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.py')\n",
    "\n",
    "password = config['postgresql']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2\n",
    "DATABASE_URI = f'postgresql://postgres:{password}@localhost:5432/home_price_post_db'\n",
    "connection = psycopg2.connect(DATABASE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'post_home_prices_allcolumn'\n",
    "\n",
    "# Using 'pandas.read_sql()' function to import the table into a DataFrame\n",
    "df = pd.read_sql(f\"SELECT * FROM {table_name}\", connection)\n",
    "\n",
    "# Close the database connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of housing_df to use in neural networks\n",
    "nn_df = df.copy()\n",
    "# nn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "dummy =  nn_df.dtypes[nn_df.dtypes == \"object\"].index.tolist()\n",
    "df_dummies = pd.get_dummies(nn_df, columns=dummy)\n",
    "df_dummies = df_dummies.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers in SalePrice\n",
    "quartiles = df_dummies['SalePrice'].quantile([0.25, 0.75])\n",
    "iqr = quartiles[0.75] - quartiles[0.25]\n",
    "\n",
    "# compute lower and upper bounds\n",
    "ll = quartiles[0.25] - 1.5*iqr\n",
    "ul = quartiles[0.75] + 1.5*iqr\n",
    "\n",
    "# Define a new column to filter out outliers\n",
    "df_dummies['norm_price'] = df_dummies['SalePrice']\n",
    "\n",
    "# filter out the outliers\n",
    "df_dummies = df_dummies.loc[(df_dummies['norm_price'] >= ll) &\n",
    "                            (df_dummies['SalePrice'] <= ul)]\n",
    "#drop the column norm_price\n",
    "df_dummies = df_dummies.drop('norm_price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.describe().transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of SalePrice without the outliers\n",
    "plt.hist(df_dummies['SalePrice'], bins=20)\n",
    "plt.title('Home Prices as Dummies with outliers')\n",
    "plt.xlabel('Sale Price')\n",
    "plt.show()\n",
    "plt.boxplot(df_dummies['SalePrice'])\n",
    "plt.title('Home Prices as Dummies with outliers')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = df_dummies['SalePrice'].values\n",
    "X = df_dummies.drop(['SalePrice'],axis=1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Deep Nueral Network to Predict Housing Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_test, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "from keras.layers import LeakyReLU\n",
    "number_input_features = len(X_train_scaled[0]) #len(train_features.columns)\n",
    "hidden_nodes_layer1 =  250\n",
    "hidden_nodes_layer2 = 150\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"LeakyReLU\"))\n",
    "\n",
    "# third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"relu\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# fit_model = nn.fit(train_features, train_labels, epochs=100)\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=150, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the model\n",
    "# test_predictions = nn.predict(test_features).flatten()\n",
    "y_pred = nn.predict(X_test_scaled).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Values [SalePrice]')\n",
    "plt.ylabel('Predictions [SalePrice]')\n",
    "lims = [30000, 350000]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error distribution\n",
    "# error = test_predictions - test_labels\n",
    "error = y_pred - y_test\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Deviations (SalePrice)')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Results from the Hyperparameter tuning to fit a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 =  43\n",
    "hidden_nodes_layer2 = 221\n",
    "hidden_nodes_layer3 = 243\n",
    "hidden_nodes_layer4 = 225\n",
    "hidden_nodes_layer5 = 35\n",
    "hidden_nodes_layer6 = 187\n",
    "\n",
    "\n",
    "nn_tuned = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_tuned.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_tuned.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# third hidden layer\n",
    "nn_tuned.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# Fourth hidden layer\n",
    "nn_tuned.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
    "\n",
    "# Fifth hidden layer\n",
    "nn_tuned.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"relu\"))\n",
    "\n",
    "# Sixth hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer6, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_tuned.add(tf.keras.layers.Dense(units=1, activation=\"relu\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_tuned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "nn_tuned.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "fit_model_tuned = nn_tuned.fit(X_train_scaled, y_train, epochs=100, validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_tuned.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the validation results\n",
    "plt.plot(fit_model_tuned.history['loss'], label='loss')\n",
    "plt.plot(fit_model_tuned.history['val_loss'], label='val_loss')\n",
    "plt.plot(fit_model_tuned.history['rmse'], label='rmse')\n",
    "# plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh', 'LeakyReLU', 'sigmoid', 'softmax'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=256,\n",
    "        step=2), activation=activation, input_dim=214))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=256,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"relu\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=kt.Objective('loss', direction='min'),\n",
    "    max_epochs=10,\n",
    "    overwrite = True,\n",
    "    directory='project',\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=10,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "top_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "top_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model.values for model in top_hyper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = tuner.get_best_models(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mod_hyp = tuner.get_best_hyperparameters(1)\n",
    "top_mod_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model[0].evaluate(X_test_scaled,y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = top_model[0].predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = (y_pred-y_test.reshape(-1,1))/y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RUTDataViz2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
